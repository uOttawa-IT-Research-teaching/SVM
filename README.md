# SVM

Comment: Please see the new read me text below between ** **

**
This machine learning training explores the power and versatility of Support Vector Machines (SVMs), a class of models that employ mathematical optimization to find maximum margin hyperplanes for classification. While SVMs are considered optimal statistical learners, some of their mechanisms like kernels have been conceptually linked to emulating heuristic policies. This training provides a robust understanding of SVMs and their applications in various classification tasks. This tutorial will encompass an overview of the learning objectives, a dedicated section detailing the prerequisites for completing the tutorial, and a guide on best practices for Research Data Management (RDM).

The training comprises four notebooks: Regularization, Kernels, Deeper-Understanding, and Noise_Red_Reduction. The first notebook, Regularization, delves into SVMs, focusing on maximizing the margin to achieve robust classification performance. The second notebook, Kernels, introduces kernel functions that transform non-linear problems into linearly separable ones, enhancing classifier accuracy. The third notebook, A Deeper Understanding, explores how diverse kernel functions in SVM classifiers affect decision boundaries and margins, offering practical insights beyond mathematical analysis.
The fourth notebook, Noise_Red_Reduction, focuses on addressing noise and variance in data, essential challenges in real-world systems. It explores how Random Forests employ ensemble modeling to reduce overfitting in SVM models. By averaging probabilistic predictions from an ensemble of SVM classifiers, Random Forests mitigate variance and enhance model performance. This training, therefore, offers insights into ensemble learning techniques and the synergy between SVM and Random Forest classifiers in handling noisy data.
**

Comment: Please remove paragraphs below if the one above works.

Support vector machines (SVMs) are powerful machine learning models that employ mathematical optimization to find maximum margin hyperplanes for classification. While SVMs are considered optimal statistical learners, some of their mechanisms like kernels have been conceptually linked to emulating heuristic policies. 


In a new paper soon to be published, we formally investigate whether SVMs can exhibit both optimal and heuristic classification behaviours, drawing parallels to dual-process psychology models like Kahneman and Tverskyâ€™s System 1 and System 2 thinking. Through theoretical analysis and experiments on synthetic and real-world datasets, we characterize conditions under which SVMs simulate heuristic judgments, including effects of non-linear kernels, training data properties, model complexity, and more. Our study reveals intriguing similarities between human and machine classification policies, despite their fundamentally different learning processes. We discuss implications for interpreting modern AI through cognitive psychology lenses while identifying key differences. This multidisciplinary work aims to provide novel empirical insights on the interplay between heuristic and optimal policies in an important class of machine learning algorithms. The results shed light on developing human-aligned classifiers that balance the strengths of both System 1 and System 2 thinking. 
